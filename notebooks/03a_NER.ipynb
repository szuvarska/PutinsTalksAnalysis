{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-07T17:52:11.138185Z",
     "start_time": "2026-01-07T17:52:03.447590Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "\n",
    "from utils import ResourceTracker, set_global_seed, ExperimentLogger\n",
    "from nlp_models import load_ner_pipeline, extract_countries_bert\n",
    "from llm_client import GeminiClient\n",
    "\n",
    "# Setup\n",
    "logger = ExperimentLogger()\n",
    "set_global_seed(42)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging experiments to: reports/experiment_20260107_185211.json\n",
      "Global seed set to: 42\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Load Data\n",
    "We load the processed speeches."
   ],
   "id": "8f508c8f511eb18"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T17:48:25.260783Z",
     "start_time": "2026-01-07T17:48:24.406182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming data is in root/data\n",
    "try:\n",
    "    df = pd.read_csv('../data/putins_talks_prepared.csv')\n",
    "    # Convert date for sorting/filtering\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    print(f\"Loaded {len(df)} rows.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Data file not found. Please ensure data/putins_talks_prepared.csv exists.\")\n",
    "df.head(2)"
   ],
   "id": "e5c1044446b99b53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5079 rows.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                 date persons  \\\n",
       "0 2012-05-07 12:20:00      []   \n",
       "1 2012-05-08 16:00:00      []   \n",
       "\n",
       "                               transcript_unfiltered  kremlin_id  \\\n",
       "0  The ceremony opened with the Russian State Fla...     15224.0   \n",
       "1  State Duma deputies approved Dmitry Medvedev a...     15266.0   \n",
       "\n",
       "                 place                                              title  \\\n",
       "0  The Kremlin, Moscow  Vladimir Putin inaugurated as President of Russia   \n",
       "1               Moscow                         State Duma plenary session   \n",
       "\n",
       "                                              teaser               tags  \\\n",
       "0  The inauguration ceremony took place in the Gr...                 []   \n",
       "1  Vladimir Putin presented the candidacy of Dmit...  ['Civil service']   \n",
       "\n",
       "                                 transcript_filtered  \\\n",
       "0  Citizens of Russia, friends, The inauguration ...   \n",
       "1  Mr Naryshkin, deputies of the Russian parliame...   \n",
       "\n",
       "                                            wordlist  \\\n",
       "0  ['citizen', 'of', 'Russia', ',', 'friend', ','...   \n",
       "1  ['Mr', 'Naryshkin', ',', 'deputy', 'of', 'the'...   \n",
       "\n",
       "                         grouped_tages  \n",
       "0                                   []  \n",
       "1  ['State_Governance_Public_Service']  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>persons</th>\n",
       "      <th>transcript_unfiltered</th>\n",
       "      <th>kremlin_id</th>\n",
       "      <th>place</th>\n",
       "      <th>title</th>\n",
       "      <th>teaser</th>\n",
       "      <th>tags</th>\n",
       "      <th>transcript_filtered</th>\n",
       "      <th>wordlist</th>\n",
       "      <th>grouped_tages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-05-07 12:20:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>The ceremony opened with the Russian State Fla...</td>\n",
       "      <td>15224.0</td>\n",
       "      <td>The Kremlin, Moscow</td>\n",
       "      <td>Vladimir Putin inaugurated as President of Russia</td>\n",
       "      <td>The inauguration ceremony took place in the Gr...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Citizens of Russia, friends, The inauguration ...</td>\n",
       "      <td>['citizen', 'of', 'Russia', ',', 'friend', ','...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-05-08 16:00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>State Duma deputies approved Dmitry Medvedev a...</td>\n",
       "      <td>15266.0</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>State Duma plenary session</td>\n",
       "      <td>Vladimir Putin presented the candidacy of Dmit...</td>\n",
       "      <td>['Civil service']</td>\n",
       "      <td>Mr Naryshkin, deputies of the Russian parliame...</td>\n",
       "      <td>['Mr', 'Naryshkin', ',', 'deputy', 'of', 'the'...</td>\n",
       "      <td>['State_Governance_Public_Service']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. BERT Extraction (Traditional Pipeline)\n",
    "We extract all mentions (duplicates included) and normalize them using our helper functions.\n"
   ],
   "id": "868baa86b556491d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T17:48:32.944726Z",
     "start_time": "2026-01-07T17:48:25.261706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model (cached)\n",
    "ner_pipe = load_ner_pipeline()\n",
    "\n",
    "# Run on a random sample first\n",
    "sample_size = 10\n",
    "df_sample = df.sample(n=min(sample_size, len(df)), random_state=42).copy()\n",
    "\n",
    "print(f\"Processing {len(df_sample)} speeches with BERT...\")\n",
    "\n",
    "with ResourceTracker(\"BERT Extraction\") as tracker:\n",
    "    # Apply extraction row by row\n",
    "    df_sample['bert_countries'] = df_sample['transcript_filtered'].apply(\n",
    "        lambda x: extract_countries_bert(x, ner_pipe)\n",
    "    )\n",
    "\n",
    "logger.log_operation(\"BERT Extraction\", tracker.duration, tracker.peak_memory_mb)\n",
    "\n",
    "# Show an example of what was found\n",
    "print(\"Example Output (List of countries found):\")\n",
    "print(df_sample['bert_countries'].iloc[0])\n"
   ],
   "id": "9da3ba9b26af7b07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NER model: dslim/bert-base-NER on device 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10 speeches with BERT...\n",
      "[BERT Extraction] Finished.\n",
      "   Duration: 6.8977 seconds\n",
      "   Peak Memory: 14.72 MB\n",
      "Example Output (List of countries found):\n",
      "['Syria', 'Turkey', 'Iran', 'Syria', 'Russia', 'Turkey', 'Iran', 'Syria', 'Syria', 'Syria', 'Iran', 'Turkey', 'Syria', 'Syria']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Modern Pipeline (LLM) vs Manual vs BERT",
   "id": "f603c2a70ac1b644"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T17:48:32.951343Z",
     "start_time": "2026-01-07T17:48:32.945723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare file for pasting into Gemini Chat\n",
    "cleared_df_sample = df_sample[['date', 'transcript_filtered']].copy()\n",
    "cleared_df_sample.to_csv('../data/samples/ner_gemini_input_sample.csv', index=False)"
   ],
   "id": "7f4562f2412d31b2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T17:48:32.957815Z",
     "start_time": "2026-01-07T17:48:32.952247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare for manual annotation and pasting Gemini results\n",
    "df_sample['manual_countries'] = \"\" # Empty column to fill\n",
    "df_sample = df_sample[['date', 'transcript_filtered', 'bert_countries', 'manual_countries']]\n",
    "\n",
    "# Save to CSV\n",
    "output_path = '../data/samples/ner_validation_sample.csv'\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "df_sample.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved validation sample to {output_path})\")"
   ],
   "id": "4c5f5e06abbf3aeb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved validation sample to ../data/samples/ner_validation_sample.csv)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Evaluation Logic",
   "id": "17be05bd4a2fed31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T17:55:40.822326Z",
     "start_time": "2026-01-07T17:55:40.809882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reload annotated data\n",
    "try:\n",
    "    annotated_df = pd.read_csv('../data/samples/ner_validation_sample_annotated.csv')\n",
    "    \n",
    "    def parse_list_like(x):\n",
    "        \"\"\"Return a list preserving duplicates for inputs that may be:\n",
    "           - a list\n",
    "           - a string like \"['A', 'B']\"\n",
    "           - a comma-separated string \"A, B, A\"\n",
    "           - NaN/empty -> []\n",
    "        \"\"\"\n",
    "        if pd.isna(x):\n",
    "            return []\n",
    "        if isinstance(x, list):\n",
    "            return x\n",
    "        s = str(x).strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        # try literal_eval for python-list-like strings\n",
    "        try:\n",
    "            val = ast.literal_eval(s)\n",
    "            if isinstance(val, list):\n",
    "                return [str(i).strip() for i in val if str(i).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "        # fallback: comma-separated\n",
    "        return [part.strip() for part in s.split(',') if part.strip()]\n",
    "\n",
    "    def calculate_metrics(pred_list, true_str):\n",
    "        true_list = parse_list_like(true_str)\n",
    "        pred_list = parse_list_like(pred_list)\n",
    "    \n",
    "        true_counter = Counter(true_list)\n",
    "        pred_counter = Counter(pred_list)\n",
    "    \n",
    "        # true positives = sum of minimum counts per item\n",
    "        tp = sum((true_counter & pred_counter).values())\n",
    "        total_pred = sum(pred_counter.values())\n",
    "        total_true = sum(true_counter.values())\n",
    "    \n",
    "        fp = total_pred - tp\n",
    "        fn = total_true - tp\n",
    "    \n",
    "        # If both empty, treat as perfect match\n",
    "        if total_pred == 0 and total_true == 0:\n",
    "            return 1.0, 1.0, 1.0\n",
    "    \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        return precision, recall, f1\n",
    "\n",
    "    # Calculate scores\n",
    "    llm_columns = [\"bert_countries\", \"gemini_countries_1\", \"gemini_countries_2\", \"gemini_countries_3\"]\n",
    "    metrics_rows = []\n",
    "    for col in llm_columns:\n",
    "        if col not in annotated_df.columns:\n",
    "            continue\n",
    "        metrics = annotated_df.apply(lambda row: calculate_metrics(row[col], row.get('manual_countries', '')), axis=1)\n",
    "        precision_mean = metrics.apply(lambda x: x[0]).mean()\n",
    "        recall_mean = metrics.apply(lambda x: x[1]).mean()\n",
    "        f1_mean = metrics.apply(lambda x: x[2]).mean()\n",
    "        metrics_rows.append({'method': col, 'precision': precision_mean, 'recall': recall_mean, 'f1': f1_mean})\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_rows, columns=['method', 'precision', 'recall', 'f1'])\n",
    "    print(metrics_df)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not load/parse annotated file: {e}\")"
   ],
   "id": "28fabb2e17a7195b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               method  precision    recall        f1\n",
      "0      bert_countries   0.831232  0.781806  0.802073\n",
      "1  gemini_countries_1   0.979167  0.994845  0.986625\n",
      "2  gemini_countries_2   0.822648  0.768397  0.770107\n",
      "3  gemini_countries_3   0.974196  0.923785  0.935087\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T17:12:41.441553Z",
     "start_time": "2026-01-07T17:12:41.440185Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b48cea2f2b3f4d3d",
   "outputs": [],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
