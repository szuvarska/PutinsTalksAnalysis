{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-07T22:55:51.009823Z",
     "start_time": "2026-01-07T22:55:44.548141Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src to path to import local modules\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "\n",
    "# Import your custom modules\n",
    "from utils import ResourceTracker, set_global_seed, ExperimentLogger\n",
    "from nlp_models import (\n",
    "    load_zero_shot_pipeline, \n",
    "    extract_context_sentences, \n",
    "    classify_sentences_batch, \n",
    "    get_hf_model_metadata,\n",
    "    get_accompanying_terms\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "# Initialize Logging and Seeding\n",
    "logger = ExperimentLogger(log_dir=\"../reports\")\n",
    "set_global_seed(42)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging experiments to: ../reports/experiment_20260107_235551.json\n",
      "Global seed set to: 42\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Finding top countries",
   "id": "99f048d95e8c5446"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T22:23:27.841151Z",
     "start_time": "2026-01-07T22:23:26.901999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Finding top countries\n",
    "df = pd.read_csv(\"../data/processed/putins_talks_with_countries.csv\")\n",
    "all_countries = df['extracted_countries'].apply(literal_eval).explode().dropna()\n",
    "country_counts = all_countries[all_countries != 'Russia'].value_counts()\n",
    "country_counts"
   ],
   "id": "6dd1e2fcc9956a70",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "extracted_countries\n",
       "Ukraine           3838\n",
       "China             2964\n",
       "Syria             2384\n",
       "United States     1646\n",
       "Turkey            1468\n",
       "                  ... \n",
       "Greenland            1\n",
       "Virgin Islands       1\n",
       "Seychelles           1\n",
       "Samoa                1\n",
       "Bhutan               1\n",
       "Name: count, Length: 170, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T22:23:27.845675Z",
     "start_time": "2026-01-07T22:23:27.842088Z"
    }
   },
   "cell_type": "code",
   "source": "country_counts.to_csv(\"../data/processed/country_counts.csv\")",
   "id": "38dfbc077fda324c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T22:23:27.849336Z",
     "start_time": "2026-01-07T22:23:27.846657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_countries = country_counts.head(5).index.tolist()\n",
    "top_countries"
   ],
   "id": "716b44b443820471",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ukraine', 'China', 'Syria', 'United States', 'Turkey']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Extracting sentences mentioning chosen countries",
   "id": "99a60b7a2bf3c3d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T22:55:51.013337Z",
     "start_time": "2026-01-07T22:55:51.010871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create target terms\n",
    "target_terms = {\n",
    "    'Poland': ['Poland', 'Polish', 'Poles'],\n",
    "    'Ukraine': ['Ukraine', 'Ukrainian', 'Ukrainians'],\n",
    "    'United States': ['United States', 'USA', 'American', 'Americans', 'U.S.'],\n",
    "    'China': ['China', 'Chinese'],\n",
    "    'Syria': ['Syria', 'Syrian'],\n",
    "    'Russia': ['Russia', 'Russian', 'Russians', 'Soviets', 'Soviet Union', 'USSR', 'Soviet', 'Russian Federation']\n",
    "}"
   ],
   "id": "37b26a159127b13f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T22:50:15.708564Z",
     "start_time": "2026-01-07T22:24:42.452462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_countries.append('Poland')\n",
    "for key, terms in target_terms.items():\n",
    "    print(f\"Processing country: {key}\")\n",
    "    # Extract context sentences\n",
    "    with ResourceTracker(\"Sentence Extraction\") as rt:\n",
    "        extracted_data = extract_context_sentences(\n",
    "            texts=df['transcript_filtered'].tolist(), \n",
    "            dates=df['date'].tolist(),\n",
    "            target_terms=terms,\n",
    "            spacy_model_name=\"en_core_web_sm\"\n",
    "        )\n",
    "    logger.log_operation(\"Sentence_Extraction\", rt.duration, rt.peak_memory_mb, {\"count\": len(extracted_data)})\n",
    "    df_sentences = pd.DataFrame(extracted_data)\n",
    "    print(f\"Found {len(df_sentences)} relevant sentences.\")\n",
    "    df_sentences.to_csv(f\"../data/sentences/sentences_{key}.csv\", index=False)\n",
    "    df_sentences.head()"
   ],
   "id": "1d148603fe17ce5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing country: Poland\n",
      "[Sentence Extraction] Finished.\n",
      "   Duration: 258.9408 seconds\n",
      "   Peak Memory: 527.33 MB\n",
      "Found 361 relevant sentences.\n",
      "Processing country: Ukraine\n",
      "[Sentence Extraction] Finished.\n",
      "   Duration: 255.7312 seconds\n",
      "   Peak Memory: 525.70 MB\n",
      "Found 3488 relevant sentences.\n",
      "Processing country: United States\n",
      "[Sentence Extraction] Finished.\n",
      "   Duration: 255.0165 seconds\n",
      "   Peak Memory: 525.84 MB\n",
      "Found 3478 relevant sentences.\n",
      "Processing country: China\n",
      "[Sentence Extraction] Finished.\n",
      "   Duration: 255.6271 seconds\n",
      "   Peak Memory: 525.72 MB\n",
      "Found 2721 relevant sentences.\n",
      "Processing country: Syria\n",
      "[Sentence Extraction] Finished.\n",
      "   Duration: 253.4403 seconds\n",
      "   Peak Memory: 536.38 MB\n",
      "Found 2213 relevant sentences.\n",
      "Processing country: Russia\n",
      "[Sentence Extraction] Finished.\n",
      "   Duration: 254.2516 seconds\n",
      "   Peak Memory: 526.76 MB\n",
      "Found 32359 relevant sentences.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Finding accompanying terms",
   "id": "37f00e79660d4c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T03:23:14.961347Z",
     "start_time": "2026-01-07T23:02:02.147210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key, terms in target_terms.items():\n",
    "    print(f\"Processing country: {key}\")\n",
    "    df_sentences = pd.read_csv(f\"../data/sentences/sentences_{key}.csv\")\n",
    "    all_accompanying_terms = []\n",
    "    for sentence in df_sentences['sentence']:\n",
    "        accompanying_terms = get_accompanying_terms(\n",
    "            sentence, \n",
    "            terms\n",
    "        )\n",
    "        all_accompanying_terms.append(accompanying_terms)\n",
    "    df_sentences['accompanying_terms'] = all_accompanying_terms\n",
    "    df_sentences.to_csv(f\"../data/sentences/sentences_{key}_with_terms.csv\", index=False)"
   ],
   "id": "884b5de16243ebfe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing country: Poland\n",
      "Processing country: Ukraine\n",
      "Processing country: United States\n",
      "Processing country: China\n",
      "Processing country: Syria\n",
      "Processing country: Russia\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Classifying sentences using zero-shot classification",
   "id": "882ec8dc684e2998"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T03:24:38.175867Z",
     "start_time": "2026-01-08T03:23:14.962367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "LABELS_BASE = [\"partner\", \"enemy\", \"neutral\"]\n",
    "LABELS_SYNONYMS = [\"ally\", \"adversary\", \"unbiased\"]\n",
    "\n",
    "# Load Model\n",
    "model_name = \"facebook/bart-large-mnli\"\n",
    "print(f\"Loading model: {model_name}...\")\n",
    "zs_pipeline = load_zero_shot_pipeline(model_name)\n",
    "\n",
    "# Log Model Metadata for Report\n",
    "meta = get_hf_model_metadata(zs_pipeline)\n",
    "print(f\"Model Architecture: {meta['model_architecture']}\")\n",
    "print(f\"Commit Hash: {meta['commit_hash']}\")"
   ],
   "id": "c4ba7600e4d96bf7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: facebook/bart-large-mnli...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6187c3107fc24d8b9962f902b4c6551b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "278c766f09de4b48b02e88f9541460e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "140d82635773458c9c838667ff06f560"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0bca5075e5fb43ee965d7404d9a8f5ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "385605028a524ad8a4b9a756392c4822"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "060ba571223f48d69d2207d49867781e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture: BartForSequenceClassification\n",
      "Commit Hash: d7645e127eaf1aefc7862fd59a17a5aa8558b8ce\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T09:36:46.124596Z",
     "start_time": "2026-01-08T06:41:46.060855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key, terms in target_terms.items():\n",
    "    print(f\"Classifying sentences for country: {key}\")\n",
    "    df_sentences = pd.read_csv(f\"../data/sentences/sentences_{key}_with_terms.csv\")\n",
    "    \n",
    "    if key == 'Russia':\n",
    "        LABELS_BASE = [\"victim\", \"leader\", \"defender\"]\n",
    "        LABELS_SYNONYMS = [\"casualty\", \"commander\", \"protector\"]\n",
    "    \n",
    "    # Classify sentences in batches\n",
    "    print(\"Running Zero-Shot (Base Labels)...\")\n",
    "    with ResourceTracker(\"ZeroShot_BaseLabels\") as rt:\n",
    "        results_base = classify_sentences_batch(\n",
    "            zs_pipeline, \n",
    "            df_sentences['sentence'].tolist(), \n",
    "            LABELS_BASE\n",
    "        )\n",
    "    \n",
    "    logger.log_operation(\n",
    "        name=\"ZeroShot_Base\",\n",
    "        duration=rt.duration,\n",
    "        memory_mb=rt.peak_memory_mb,\n",
    "        metrics={\"model\": model_name, \"labels\": LABELS_BASE, \"hash\": meta['commit_hash']}\n",
    "    )\n",
    "    \n",
    "    df_sentences['zs_base_label'] = [r['top_label'] for r in results_base]\n",
    "    df_sentences['zs_base_score'] = [r['top_score'] for r in results_base]\n",
    "    \n",
    "    print(\"Running Zero-Shot (Synonym Labels)...\")\n",
    "    with ResourceTracker(\"ZeroShot_SynonymLabels\") as rt:\n",
    "        results_syn = classify_sentences_batch(\n",
    "            zs_pipeline, \n",
    "            df_sentences['sentence'].tolist(), \n",
    "            LABELS_SYNONYMS)\n",
    "    \n",
    "    logger.log_operation(\n",
    "        name=\"ZeroShot_Synonyms\",\n",
    "        duration=rt.duration,\n",
    "        memory_mb=rt.peak_memory_mb,\n",
    "        metrics={\"model\": model_name, \"labels\": LABELS_SYNONYMS}\n",
    "    )\n",
    "    \n",
    "    df_sentences['zs_syn_label'] = [r['top_label'] for r in results_syn]\n",
    "    df_sentences['zs_syn_score'] = [r['top_score'] for r in results_syn]\n",
    "    \n",
    "    # Save classified results\n",
    "    df_sentences.to_csv(f\"../data/sentences/sentences_{key}_classified.csv\", index=False)\n",
    "    print(f\"Saved classified sentences for {key}.\")"
   ],
   "id": "3779662b0ab41b60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying sentences for country: Poland\n",
      "Running Zero-Shot (Base Labels)...\n",
      "[ZeroShot_BaseLabels] Finished.\n",
      "   Duration: 45.8916 seconds\n",
      "   Peak Memory: 0.77 MB\n",
      "Running Zero-Shot (Synonym Labels)...\n",
      "[ZeroShot_SynonymLabels] Finished.\n",
      "   Duration: 45.2267 seconds\n",
      "   Peak Memory: 0.35 MB\n",
      "Saved classified sentences for Poland.\n",
      "Classifying sentences for country: Ukraine\n",
      "Running Zero-Shot (Base Labels)...\n",
      "[ZeroShot_BaseLabels] Finished.\n",
      "   Duration: 415.3089 seconds\n",
      "   Peak Memory: 3.53 MB\n",
      "Running Zero-Shot (Synonym Labels)...\n",
      "[ZeroShot_SynonymLabels] Finished.\n",
      "   Duration: 414.4275 seconds\n",
      "   Peak Memory: 3.36 MB\n",
      "Saved classified sentences for Ukraine.\n",
      "Classifying sentences for country: United States\n",
      "Running Zero-Shot (Base Labels)...\n",
      "[ZeroShot_BaseLabels] Finished.\n",
      "   Duration: 412.3324 seconds\n",
      "   Peak Memory: 3.49 MB\n",
      "Running Zero-Shot (Synonym Labels)...\n",
      "[ZeroShot_SynonymLabels] Finished.\n",
      "   Duration: 412.1403 seconds\n",
      "   Peak Memory: 3.35 MB\n",
      "Saved classified sentences for United States.\n",
      "Classifying sentences for country: China\n",
      "Running Zero-Shot (Base Labels)...\n",
      "[ZeroShot_BaseLabels] Finished.\n",
      "   Duration: 321.7130 seconds\n",
      "   Peak Memory: 2.75 MB\n",
      "Running Zero-Shot (Synonym Labels)...\n",
      "[ZeroShot_SynonymLabels] Finished.\n",
      "   Duration: 321.4236 seconds\n",
      "   Peak Memory: 2.62 MB\n",
      "Saved classified sentences for China.\n",
      "Classifying sentences for country: Syria\n",
      "Running Zero-Shot (Base Labels)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ZeroShot_BaseLabels] Finished.\n",
      "   Duration: 263.4440 seconds\n",
      "   Peak Memory: 2.21 MB\n",
      "Running Zero-Shot (Synonym Labels)...\n",
      "[ZeroShot_SynonymLabels] Finished.\n",
      "   Duration: 263.1236 seconds\n",
      "   Peak Memory: 2.14 MB\n",
      "Saved classified sentences for Syria.\n",
      "Classifying sentences for country: Russia\n",
      "Running Zero-Shot (Base Labels)...\n",
      "[ZeroShot_BaseLabels] Finished.\n",
      "   Duration: 3792.1597 seconds\n",
      "   Peak Memory: 33.50 MB\n",
      "Running Zero-Shot (Synonym Labels)...\n",
      "[ZeroShot_SynonymLabels] Finished.\n",
      "   Duration: 3792.4268 seconds\n",
      "   Peak Memory: 31.09 MB\n",
      "Saved classified sentences for Russia.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Classifying sentences using Gemini and human annotation",
   "id": "4b2ae5dde136ab8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. Classification evaluation",
   "id": "3014aba50de741ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 7. Aggregating and saving classification counts",
   "id": "a8934199b3b345dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def aggregate_counts(df, method_col, method_name):\n",
    "    \"\"\"Counts occurrences of each label per country for a given method.\"\"\"\n",
    "    # Group by Country (found_term) and Label\n",
    "    counts = df.groupby(['found_term', method_col]).size().reset_index(name='count')\n",
    "    counts['method'] = method_name\n",
    "    counts.rename(columns={method_col: 'label'}, inplace=True)\n",
    "    return counts"
   ],
   "id": "a7959a3d983e42c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# List of method columns to aggregate\n",
    "methods_to_count = [\n",
    "    ('zs_base_label', 'ZeroShot_Base'),\n",
    "    ('zs_syn_label', 'ZeroShot_Synonym'),\n",
    "    ('ai_prompt_a', 'AI_Prompt_A'),\n",
    "    ('ai_prompt_b', 'AI_Prompt_B')\n",
    "]\n",
    "\n",
    "all_counts = []\n",
    "\n",
    "for col, name in methods_to_count:\n",
    "    if col in df_sentences.columns:\n",
    "        all_counts.append(aggregate_counts(df_sentences, col, name))\n",
    "\n",
    "# Combine all into one DataFrame\n",
    "if all_counts:\n",
    "    df_counts = pd.concat(all_counts, ignore_index=True)\n",
    "    \n",
    "    # Reorder for clarity: Country, Method, Label, Count\n",
    "    df_counts = df_counts[['found_term', 'method', 'label', 'count']]\n",
    "    \n",
    "    print(\"Summary of Classification Counts:\")\n",
    "    display(df_counts.head(15))\n",
    "    \n",
    "    # Save\n",
    "    df_counts.to_csv(\"../data/processed/country_label_counts.csv\", index=False)\n",
    "else:\n",
    "    print(\"No classification columns found to aggregate.\")"
   ],
   "id": "c5756473e499ed10"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
